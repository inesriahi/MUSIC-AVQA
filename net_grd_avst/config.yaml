system:
  sys_path: "/home/guangyao_li/projects/avqa/music_avqa_camera_ready"
  warnings: ignore
  tensorboard_logdir: "runs/net_avst/"
  cuda_visible_devices: "0, 1"

global:
  seed: 1

directories:
  audio_dir: "/scratch/project_462000189/datasets/MUCIS-AVQA/AVQA_Dataset/raw_audio"
  video_res14x14_dir: "/scratch/project_462000189/datasets/MUCIS-AVQA/AVQA_Dataset/video_frames"
  label_train: "/scratch/project_462000189/datasets/MUCIS-AVQA/json/avqa-train.json"
  label_val: "/scratch/project_462000189/datasets/MUCIS-AVQA/json/avqa-val.json"
  label_test: "/scratch/project_462000189/datasets/MUCIS-AVQA/json/avqa-test.json"
  model_save_dir: "net_grd_avst/avst_models/"
  pretrained_file: "grounding_gen/models_grounding_gen/main_grounding_gen_best.pt"

model:
  name: "AVQA_Fusion_Net"
  checkpoint: "avst"
  mode: "train"
  visual_encoder: 
    # model_name: "vit_base_patch16_224_in21k"
    # is_frozen: True

    # model_name: "resnet18"
    # is_frozen: True
  audio_encoder:
    # model_name: "vit_base_patch16_224_in21k"
    # is_frozen: True
  common_encoder: # leave this empty if you want to use different encoders for vision and audio
    model_name: "vit_base_patch16_224_in21k"
    is_frozen: True

optimizer:
  learning_rate: 0.0001
  step_lr:
    step_size: 8
    gamma: 0.1

train:
  epochs: 50
  log_interval: 50
  loader:
    audio_shape: 
      channels: 3
      num_mel_bins: 224
      num_frames: 224
    visual_shape:
      channels: 3
      width: 224
      height: 224
    batch_size: 16
    num_workers: 4

val:
  loader:
    audio_shape: 
      channels: 3
      num_mel_bins: 224
      num_frames: 224
    visual_shape:
      channels: 3
      width: 224
      height: 224
    batch_size: 1
    num_workers: 4
      
test:
  loader:
    audio_shape: 
      channels: 3
      num_mel_bins: 224
      num_frames: 224
    visual_shape:
      channels: 3
      width: 224
      height: 224
    batch_size: 1
    num_workers: 4
